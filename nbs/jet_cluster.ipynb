{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "plt.style.use(\"dark_paper\")\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n",
    "\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color,cmap=\"Dark2\")\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_graph(h, color, epoch=None, loss=None):\n",
    "    fig,ax = plt.subplots(1,1,figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                         node_color=color,edge_color=\"w\", cmap=\"Set2\")\n",
    "    return fig,ax\n",
    "def _process(data): \n",
    "    x=data.loc[:,[\"E\",\"px\",\"py\",\"pz\"]].to_numpy()\n",
    "    y=data.loc[:,[\"mask\"]].to_numpy()\n",
    "    data = Data(x=torch.from_numpy(x).float(),\n",
    "        y=torch.from_numpy(y).float(),\n",
    "        #edge_index=dense_to_sparse(torch.ones(x.shape[0],x.shape[0]).fill_diagonal_(0))[0]#complete\n",
    "        edge_index=dense_to_sparse(torch.eye(x.shape[0]))[0]#self only\n",
    "        #edge_index=dense_to_sparse(torch.ones(x.shape[0],x.shape[0]))[0]#complete graph\n",
    "        ) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [_process(pd.read_hdf(\"data/W/evts.h5\", key=f\"evt{i}\")) for i in range(1,2000)] +\\\n",
    "[_process(pd.read_hdf(\"data/QCD/evts_qcd.h5\", key=f\"evt{i}\")) for i in range(1,2000)]\n",
    "val_set = [_process(pd.read_hdf(\"data/W/evts.h5\", key=f\"evt{i}\")) for i in range(2000,2201)]+\\\n",
    "[_process(pd.read_hdf(\"data/QCD/evts_qcd.h5\", key=f\"evt{i}\")) for i in range(2000,2201)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(edge_index=[2, 302], x=[302, 4], y=[302, 1])\n",
      "=============================================================\n",
      "Number of nodes: 302\n",
      "Number of edges: 302\n",
      "Average node degree: 1.00\n",
      "Contains isolated nodes: True\n",
      "Contains self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "data = train_set[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "#         self.conv1 = torch_geometric.nn.GCNConv(4, hidden_channels)\n",
    "#         self.conv2 = torch_geometric.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = torch_geometric.nn.GCNConv(hidden_channels, 1)\n",
    "        self.mlp1 = torch.nn.Sequential(torch.nn.Linear(4*2,hidden_channels))\n",
    "        self.mlp2 = torch.nn.Sequential(torch.nn.Linear(hidden_channels*2,hidden_channels))\n",
    "        self.mlp3 = torch.nn.Sequential(torch.nn.Linear(hidden_channels*2,hidden_channels))\n",
    "        self.mlp4 = torch.nn.Sequential(torch.nn.Linear(hidden_channels*2,hidden_channels))        \n",
    "        self.mlp5 = torch.nn.Sequential(torch.nn.Linear(hidden_channels*2,1))\n",
    "        self.conv1 = torch_geometric.nn.DynamicEdgeConv(self.mlp1,k=15,aggr=\"max\")\n",
    "        self.conv2 = torch_geometric.nn.DynamicEdgeConv(self.mlp2,k=5,aggr=\"max\")\n",
    "        self.conv3 = torch_geometric.nn.DynamicEdgeConv(self.mlp3,k=5,aggr=\"max\")\n",
    "        self.conv4 = torch_geometric.nn.DynamicEdgeConv(self.mlp4,k=5,aggr=\"max\")\n",
    "        self.conv5 = torch_geometric.nn.DynamicEdgeConv(self.mlp5,k=5,aggr=\"max\")\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(hidden_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         #x = self.batchnorm(x)\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         #x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv3(x, edge_index)\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.conv2(x).relu()\n",
    "        x = self.conv3(x).relu()\n",
    "        x = self.conv4(x).relu()\n",
    "        x = self.conv5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.from_numpy(np.concatenate([data.y.tolist() for data in train_set]))\n",
    "ones_weight = targets.sum().item()/len(targets)\n",
    "weight = 1/torch.tensor([1-ones_weight,ones_weight])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "def get_weights(targets):\n",
    "    weight_ = weight[targets.data.view(-1).long()].view_as(targets)\n",
    "    weight_ = weight_.to(targets.device)\n",
    "    return weight_\n",
    "    \n",
    "def train(data,weighted=True):\n",
    "    gcn.train()\n",
    "    weight_ = get_weights(data.y)\n",
    "    out = gcn(data.x,data.edge_index)#[data.train_mask]\n",
    "    out = torch.sigmoid(out)\n",
    "    loss = criterion(out,\n",
    "                     data.y#[data.train_mask]\n",
    "                    )\n",
    "    if weighted:\n",
    "        loss = (loss*weight_).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def val(data,weighted=True):\n",
    "    gcn.eval()\n",
    "    weight_ = get_weights(data.y)\n",
    "    with torch.no_grad():\n",
    "        out = gcn(data.x,data.edge_index)\n",
    "        out = torch.sigmoid(out)\n",
    "        loss = criterion(out,data.y)\n",
    "        if weighted:\n",
    "            loss = (loss*weight_).mean()\n",
    "        else:\n",
    "            loss = loss.mean()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 /500 | Loss 0.8895\n",
      "Epoch 51 /500 | Loss 0.4374\n",
      "Epoch 101/500 | Loss 0.2438\n",
      "Epoch 151/500 | Loss 0.1079\n",
      "Epoch 201/500 | Loss 0.0757\n",
      "Epoch 251/500 | Loss 0.0471\n",
      "Epoch 301/500 | Loss 0.0292\n",
      "Epoch 351/500 | Loss 0.0173\n",
      "Epoch 401/500 | Loss 0.0116\n",
      "Epoch 451/500 | Loss 0.0073\n"
     ]
    }
   ],
   "source": [
    "#single graph\n",
    "gcn = GCN(512)\n",
    "gcn.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(),weight_decay=5e-4,lr=1e-4)\n",
    "epochs = 500\n",
    "for epoch in range(1,epochs):\n",
    "    loss = train(train_set[0].to(device))\n",
    "    if epoch%50==1:\n",
    "        print(f\"Epoch {epoch:^3d}/{epochs} | Loss {loss:^4.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 /500 | Loss 0.3901| Val loss 0.3849\n",
      "Epoch 51 /500 | Loss 0.3030| Val loss 0.4026\n",
      "Epoch 101/500 | Loss 0.2076| Val loss 0.6491\n",
      "Epoch 151/500 | Loss 0.0929| Val loss 1.2550\n",
      "Epoch 201/500 | Loss 0.0370| Val loss 1.8134\n",
      "Epoch 251/500 | Loss 0.0176| Val loss 1.9532\n",
      "Epoch 301/500 | Loss 0.0323| Val loss 1.5949\n",
      "Epoch 351/500 | Loss 0.0521| Val loss 1.3552\n",
      "Epoch 401/500 | Loss 0.0198| Val loss 1.6480\n",
      "Epoch 451/500 | Loss 0.0153| Val loss 1.5032\n"
     ]
    }
   ],
   "source": [
    "gcn = GCN(512)\n",
    "gcn.to(device)\n",
    "criterion = torch.nn.BCELoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(gcn.parameters(),weight_decay=5e-4,lr=1e-4)\n",
    "epochs = 500\n",
    "samples = 200\n",
    "losses = []\n",
    "val_losses = []\n",
    "for epoch in range(1,epochs):\n",
    "    loss = 0\n",
    "    for data in train_set[:samples]:\n",
    "        loss += train(data.to(device))\n",
    "    loss /= samples\n",
    "    losses.append(loss)\n",
    "    if epoch%50==1:\n",
    "        loss_val = 0 \n",
    "        for data in val_set[:samples]:\n",
    "            loss_val+= val(data.to(device))\n",
    "        loss_val /= samples\n",
    "        val_losses.append(loss_val)\n",
    "        print(f\"Epoch {epoch:^3d}/{epochs} | Loss {loss:^4.4f}| Val loss {loss_val:^4.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00      3406\n",
      "         1.0       0.95      1.00      0.97       414\n",
      "\n",
      "    accuracy                           0.99      3820\n",
      "   macro avg       0.97      1.00      0.98      3820\n",
      "weighted avg       0.99      0.99      0.99      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = (np.concatenate([torch.sigmoid(gcn(data.x,data.edge_index)).detach().flatten().tolist() \n",
    "                        for data in train_set[:10]]\n",
    "                      )>.5).astype(int)\n",
    "target = np.concatenate([data.y.flatten().tolist() for data in train_set[:10]])\n",
    "print(classification_report(target,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92     37386\n",
      "         1.0       0.23      0.22      0.22      3706\n",
      "\n",
      "    accuracy                           0.86     41092\n",
      "   macro avg       0.58      0.57      0.57     41092\n",
      "weighted avg       0.86      0.86      0.86     41092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = (np.concatenate([torch.sigmoid(gcn(data.x,data.edge_index)).detach().flatten().tolist() \n",
    "                        for data in val_set[:20]]\n",
    "                      )>.5).astype(int)\n",
    "target = np.concatenate([data.y.flatten().tolist() for data in val_set[:20]])\n",
    "print(classification_report(target,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34657,  2729],\n",
       "       [ 2900,   806]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(target,pred,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pt cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class eye_GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = torch_geometric.nn.GCNConv(4, hidden_channels,add_self_loops=False)\n",
    "        self.conv2 = torch_geometric.nn.GCNConv(hidden_channels, hidden_channels,add_self_loops=False)\n",
    "        self.conv3 = torch_geometric.nn.GCNConv(hidden_channels, hidden_channels,add_self_loops=False)\n",
    "        self.conv4 = torch_geometric.nn.GCNConv(hidden_channels, hidden_channels,add_self_loops=False)\n",
    "        self.conv5 = torch_geometric.nn.GCNConv(hidden_channels, 1,add_self_loops=False)\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(hidden_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         #x = self.batchnorm(x)\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         #x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv3(x, edge_index)\n",
    "        x = self.conv1(x,edge_index).relu()\n",
    "        x = self.conv2(x,edge_index).relu()\n",
    "        x = self.conv3(x,edge_index).relu()\n",
    "        x = self.conv4(x,edge_index).relu()\n",
    "        x = self.conv5(x,edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 /500 | Loss 0.8501\n",
      "Epoch 51 /500 | Loss 0.2819\n",
      "Epoch 101/500 | Loss 0.1226\n",
      "Epoch 151/500 | Loss 0.0663\n",
      "Epoch 201/500 | Loss 1.2177\n",
      "Epoch 251/500 | Loss 0.1757\n",
      "Epoch 301/500 | Loss 0.1056\n",
      "Epoch 351/500 | Loss 0.1041\n",
      "Epoch 401/500 | Loss 0.0778\n",
      "Epoch 451/500 | Loss 0.0296\n"
     ]
    }
   ],
   "source": [
    "#single graph\n",
    "gcn = eye_GCN(256)\n",
    "gcn.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(),weight_decay=5e-4,lr=1e-2)\n",
    "epochs = 500\n",
    "for epoch in range(1,epochs):\n",
    "    loss = train(train_set[0])\n",
    "    if epoch%50==1:\n",
    "        print(f\"Epoch {epoch:^3d}/{epochs} | Loss {loss:^4.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 /1500 | Loss 0.0603| Val loss 0.0377\n",
      "Epoch 51 /1500 | Loss 0.0318| Val loss 0.0328\n",
      "Epoch 101/1500 | Loss 0.0315| Val loss 0.0325\n",
      "Epoch 151/1500 | Loss 0.0316| Val loss 0.0332\n",
      "Epoch 201/1500 | Loss 0.0318| Val loss 0.0332\n",
      "Epoch 451/1500 | Loss 0.0310| Val loss 0.0325\n",
      "Epoch 501/1500 | Loss 0.0311| Val loss 0.0325\n",
      "Epoch 551/1500 | Loss 0.0312| Val loss 0.0326\n",
      "Epoch 601/1500 | Loss 0.0310| Val loss 0.0325\n",
      "Epoch 651/1500 | Loss 0.0313| Val loss 0.0326\n",
      "Epoch 701/1500 | Loss 0.0310| Val loss 0.0326\n",
      "Epoch 751/1500 | Loss 0.0312| Val loss 0.0325\n",
      "Epoch 801/1500 | Loss 0.0311| Val loss 0.0331\n",
      "Epoch 851/1500 | Loss 0.0310| Val loss 0.0325\n",
      "Epoch 901/1500 | Loss 0.0309| Val loss 0.0326\n",
      "Epoch 951/1500 | Loss 0.0308| Val loss 0.0328\n",
      "Epoch 1001/1500 | Loss 0.0309| Val loss 0.0324\n",
      "Epoch 1051/1500 | Loss 0.0310| Val loss 0.0329\n",
      "Epoch 1101/1500 | Loss 0.0308| Val loss 0.0331\n",
      "Epoch 1151/1500 | Loss 0.0308| Val loss 0.0328\n",
      "Epoch 1201/1500 | Loss 0.0313| Val loss 0.0327\n",
      "Epoch 1251/1500 | Loss 0.0308| Val loss 0.0324\n",
      "Epoch 1301/1500 | Loss 0.0310| Val loss 0.0325\n",
      "Epoch 1351/1500 | Loss 0.0309| Val loss 0.0328\n",
      "Epoch 1401/1500 | Loss 0.0308| Val loss 0.0329\n",
      "Epoch 1451/1500 | Loss 0.0308| Val loss 0.0330\n"
     ]
    }
   ],
   "source": [
    "gcn = eye_GCN(512)\n",
    "gcn.to(device)\n",
    "criterion = torch.nn.BCELoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(gcn.parameters(),weight_decay=5e-4,lr=1e-3)\n",
    "epochs = 1500\n",
    "samples = 2000\n",
    "losses = []\n",
    "val_losses = []\n",
    "for epoch in range(1,epochs):\n",
    "    loss = 0\n",
    "    for data in train_set[:samples]:\n",
    "        loss += train(data.to(device))\n",
    "    loss /= samples\n",
    "    losses.append(loss)\n",
    "    if epoch%50==1:\n",
    "        loss_val = 0 \n",
    "        for data in val_set[:samples]:\n",
    "            loss_val+= val(data.to(device))\n",
    "        loss_val /= samples\n",
    "        val_losses.append(loss_val)\n",
    "        print(f\"Epoch {epoch:^3d}/{epochs} | Loss {loss:^4.4f}| Val loss {loss_val:^4.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.49      0.65     37386\n",
      "         1.0       0.13      0.77      0.22      3706\n",
      "\n",
      "    accuracy                           0.52     41092\n",
      "   macro avg       0.54      0.63      0.44     41092\n",
      "weighted avg       0.88      0.52      0.61     41092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = (np.concatenate([torch.sigmoid(gcn(data.x,data.edge_index)).detach().flatten().tolist() \n",
    "                        for data in val_set[:100]]\n",
    "                      )>.5).astype(int)\n",
    "target = np.concatenate([data.y.flatten().tolist() for data in val_set[:100]])\n",
    "print(classification_report(target,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18500, 18886],\n",
       "       [  854,  2852]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(target,pred,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old\n",
    "trained for 1500 epochs without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 /1000 | Loss 0.2497\n",
      "Epoch 51 /1000 | Loss 0.2832\n",
      "Epoch 101/1000 | Loss 0.2369\n",
      "Epoch 151/1000 | Loss 0.3024\n",
      "Epoch 201/1000 | Loss 0.3346\n",
      "Epoch 251/1000 | Loss 0.2763\n",
      "Epoch 301/1000 | Loss 0.1760\n",
      "Epoch 351/1000 | Loss 0.1935\n",
      "Epoch 401/1000 | Loss 0.1592\n",
      "Epoch 451/1000 | Loss 0.1584\n",
      "Epoch 501/1000 | Loss 0.1971\n",
      "Epoch 551/1000 | Loss 0.1928\n",
      "Epoch 601/1000 | Loss 0.1564\n",
      "Epoch 651/1000 | Loss 0.1390\n",
      "Epoch 701/1000 | Loss 0.1664\n",
      "Epoch 751/1000 | Loss 0.1535\n",
      "Epoch 801/1000 | Loss 0.2994\n",
      "Epoch 851/1000 | Loss 0.2604\n",
      "Epoch 901/1000 | Loss 0.2284\n",
      "Epoch 951/1000 | Loss 0.2221\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(1,epochs):\n",
    "    for data in train_set:\n",
    "        loss = train(data)\n",
    "    if epoch%50==1:\n",
    "        print(f\"Epoch {epoch:^3d}/{epochs} | Loss {loss:^4.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1462511122226715, 0.0)\n",
      "(2.850352454650726e-24, 0.0)\n",
      "(0.991333544254303, 1.0)\n",
      "(0.8665170669555664, 1.0)\n",
      "(0.8194782733917236, 1.0)\n",
      "(0.4785202741622925, 1.0)\n",
      "(0.6124110221862793, 1.0)\n",
      "(0.42588767409324646, 1.0)\n",
      "(0.9294651746749878, 1.0)\n",
      "(0.9990184307098389, 1.0)\n",
      "(0.7726152539253235, 1.0)\n",
      "(0.13509753346443176, 1.0)\n",
      "(0.9999765157699585, 1.0)\n",
      "(0.21755670011043549, 1.0)\n",
      "(0.1605711728334427, 0.0)\n",
      "(0.006053730845451355, 0.0)\n",
      "(2.066202343547696e-29, 0.0)\n",
      "(0.06368965655565262, 0.0)\n",
      "(0.023447738960385323, 0.0)\n",
      "(4.5762932797345415e-15, 0.0)\n",
      "(0.13979420065879822, 0.0)\n",
      "(0.00016625090211164206, 0.0)\n",
      "(0.01608888804912567, 0.0)\n",
      "(0.21124699711799622, 0.0)\n",
      "(0.23464731872081757, 0.0)\n",
      "(0.15603823959827423, 0.0)\n",
      "(0.051417045295238495, 0.0)\n",
      "(0.0005795211181975901, 0.0)\n",
      "(8.600208219189387e-12, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0024768030270934105, 0.0)\n",
      "(0.001288735307753086, 0.0)\n",
      "(6.665423361185767e-17, 0.0)\n",
      "(6.021772801492505e-26, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(6.632580320400472e-16, 0.0)\n",
      "(0.7826274633407593, 1.0)\n",
      "(0.4556870460510254, 1.0)\n",
      "(0.4491643011569977, 1.0)\n",
      "(0.41434812545776367, 1.0)\n",
      "(0.9299032688140869, 1.0)\n",
      "(0.7956961989402771, 1.0)\n",
      "(0.2736877501010895, 1.0)\n",
      "(0.3742833137512207, 1.0)\n",
      "(0.6772201657295227, 1.0)\n",
      "(0.17024777829647064, 0.0)\n",
      "(0.5417570471763611, 0.0)\n",
      "(0.015218963846564293, 0.0)\n",
      "(0.07608754187822342, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.007667797617614269, 0.0)\n",
      "(0.16758431494235992, 0.0)\n",
      "(0.10430198162794113, 0.0)\n",
      "(0.14326132833957672, 0.0)\n",
      "(1.9417750962702485e-08, 0.0)\n",
      "(0.00013461781782098114, 0.0)\n",
      "(0.021660182625055313, 0.0)\n",
      "(0.04342344030737877, 0.0)\n",
      "(0.0008066619047895074, 0.0)\n",
      "(0.27563703060150146, 0.0)\n",
      "(0.002424564678221941, 0.0)\n",
      "(0.0616932213306427, 0.0)\n",
      "(0.12901780009269714, 0.0)\n",
      "(0.17182689905166626, 0.0)\n",
      "(0.16408225893974304, 0.0)\n",
      "(0.156347393989563, 0.0)\n",
      "(0.08907164633274078, 0.0)\n",
      "(0.025185946375131607, 0.0)\n",
      "(0.08995560556650162, 0.0)\n",
      "(2.245348150609061e-05, 0.0)\n",
      "(0.028794920071959496, 0.0)\n",
      "(0.00012924315524287522, 0.0)\n",
      "(7.610741158714518e-05, 0.0)\n",
      "(4.055754470755346e-05, 0.0)\n",
      "(0.0006977570592425764, 0.0)\n",
      "(0.0001604045828571543, 0.0)\n",
      "(2.1231921891740058e-06, 0.0)\n",
      "(5.049671618351237e-16, 0.0)\n",
      "(8.973715577030401e-26, 0.0)\n",
      "(2.6906580262442414e-12, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.6994818449020386, 1.0)\n",
      "(0.2942550480365753, 0.0)\n",
      "(5.526320774693755e-15, 0.0)\n",
      "(0.19415965676307678, 0.0)\n",
      "(0.8745110034942627, 1.0)\n",
      "(0.514437198638916, 1.0)\n",
      "(0.3655194938182831, 1.0)\n",
      "(0.3896385431289673, 1.0)\n",
      "(0.8342751860618591, 1.0)\n",
      "(0.44094812870025635, 1.0)\n",
      "(0.19725902378559113, 1.0)\n",
      "(0.09876374155282974, 1.0)\n",
      "(0.9461100101470947, 1.0)\n",
      "(0.20651386678218842, 1.0)\n",
      "(0.14408597350120544, 0.0)\n",
      "(0.16006839275360107, 0.0)\n",
      "(0.13558465242385864, 0.0)\n",
      "(0.18619924783706665, 0.0)\n",
      "(0.11537592858076096, 0.0)\n",
      "(0.10838120430707932, 0.0)\n",
      "(3.824792482731709e-09, 0.0)\n",
      "(0.0025966898538172245, 0.0)\n",
      "(0.00481078214943409, 0.0)\n",
      "(0.004220564849674702, 0.0)\n",
      "(0.015821991488337517, 0.0)\n",
      "(0.007916712202131748, 0.0)\n",
      "(0.006923232227563858, 0.0)\n",
      "(0.013393094763159752, 0.0)\n",
      "(0.005145356059074402, 0.0)\n",
      "(0.035782791674137115, 0.0)\n",
      "(0.30005162954330444, 0.0)\n",
      "(0.25185626745224, 0.0)\n",
      "(0.027072668075561523, 0.0)\n",
      "(0.17505377531051636, 0.0)\n",
      "(0.3135101795196533, 0.0)\n",
      "(0.2097225785255432, 0.0)\n",
      "(0.24734853208065033, 0.0)\n",
      "(0.11621779948472977, 0.0)\n",
      "(0.16333961486816406, 0.0)\n",
      "(0.008004448376595974, 0.0)\n",
      "(0.00023567405878566206, 0.0)\n",
      "(0.11579478532075882, 0.0)\n",
      "(0.06829601526260376, 0.0)\n",
      "(0.004409858025610447, 0.0)\n",
      "(0.010795235633850098, 0.0)\n",
      "(0.00013279213453643024, 0.0)\n",
      "(0.000567816139664501, 0.0)\n",
      "(0.0010951149743050337, 0.0)\n",
      "(2.483496064087376e-05, 0.0)\n",
      "(0.003733827266842127, 0.0)\n",
      "(0.06718040257692337, 0.0)\n",
      "(0.0502186194062233, 0.0)\n",
      "(0.0006783795543015003, 0.0)\n",
      "(1.7102893547753787e-16, 0.0)\n",
      "(7.599306858156567e-15, 0.0)\n",
      "(1.1918356926376994e-12, 0.0)\n",
      "(3.717469652813097e-27, 0.0)\n",
      "(0.9395254254341125, 1.0)\n",
      "(0.6501314043998718, 1.0)\n",
      "(0.6182910203933716, 1.0)\n",
      "(0.3822268545627594, 1.0)\n",
      "(0.770234227180481, 1.0)\n",
      "(0.4700344502925873, 1.0)\n",
      "(0.30673009157180786, 1.0)\n",
      "(0.13622577488422394, 1.0)\n",
      "(0.9494584798812866, 1.0)\n",
      "(0.724760890007019, 1.0)\n",
      "(0.7128024101257324, 1.0)\n",
      "(0.8062326312065125, 1.0)\n",
      "(0.015824614092707634, 0.0)\n",
      "(0.007929212413728237, 0.0)\n",
      "(0.17680037021636963, 0.0)\n",
      "(0.13857851922512054, 0.0)\n",
      "(0.17899766564369202, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0026940307579934597, 0.0)\n",
      "(1.3548504540494832e-09, 0.0)\n",
      "(0.046296872198581696, 0.0)\n",
      "(0.02047242596745491, 0.0)\n",
      "(0.09089396148920059, 0.0)\n",
      "(0.15288400650024414, 0.0)\n",
      "(0.008078611455857754, 0.0)\n",
      "(0.004135861061513424, 0.0)\n",
      "(0.0008383464301005006, 0.0)\n",
      "(7.208089444874118e-16, 0.0)\n",
      "(1.7513048788941887e-09, 0.0)\n",
      "(0.00018606123921927065, 0.0)\n",
      "(6.106158669934144e-33, 0.0)\n",
      "(2.727502113053504e-10, 0.0)\n",
      "(0.23674696683883667, 0.0)\n",
      "(0.1018749475479126, 0.0)\n",
      "(0.6424106955528259, 1.0)\n",
      "(0.2794598340988159, 1.0)\n",
      "(0.9093757271766663, 1.0)\n",
      "(0.9463219046592712, 1.0)\n",
      "(0.962811291217804, 1.0)\n",
      "(0.19007763266563416, 1.0)\n",
      "(0.37455055117607117, 1.0)\n",
      "(0.5897923707962036, 1.0)\n",
      "(0.4088551104068756, 0.0)\n",
      "(0.3591490387916565, 0.0)\n",
      "(0.23114226758480072, 0.0)\n",
      "(0.1545790731906891, 0.0)\n",
      "(0.061212215572595596, 0.0)\n",
      "(0.14569894969463348, 0.0)\n",
      "(0.009921128861606121, 0.0)\n",
      "(2.8607405511138495e-06, 0.0)\n",
      "(0.004593542777001858, 0.0)\n",
      "(0.05476474016904831, 0.0)\n",
      "(0.007036732975393534, 0.0)\n",
      "(0.004170620813965797, 0.0)\n",
      "(0.3061400353908539, 0.0)\n",
      "(0.2973373830318451, 0.0)\n",
      "(0.07417315989732742, 0.0)\n",
      "(0.010852783918380737, 0.0)\n",
      "(0.0005048614111728966, 0.0)\n",
      "(0.0002524942101445049, 0.0)\n",
      "(0.20462176203727722, 1.0)\n",
      "(0.2191316783428192, 1.0)\n",
      "(0.17379797995090485, 1.0)\n",
      "(0.6538199186325073, 1.0)\n",
      "(0.0932132676243782, 0.0)\n",
      "(0.09265097230672836, 0.0)\n",
      "(0.03484456241130829, 0.0)\n",
      "(0.02891804277896881, 0.0)\n",
      "(0.694704532623291, 1.0)\n",
      "(0.45624399185180664, 1.0)\n",
      "(0.16719503700733185, 1.0)\n",
      "(0.30352577567100525, 1.0)\n",
      "(0.14997196197509766, 0.0)\n"
     ]
    }
   ],
   "source": [
    "data = train_set[8]\n",
    "print(*zip(torch.sigmoid(gcn(data.x,data.edge_index)).detach().flatten().tolist(),\n",
    "      data.y.flatten().tolist()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.97      0.94     35914\n",
      "         1.0       0.34      0.17      0.23      3688\n",
      "\n",
      "    accuracy                           0.89     39602\n",
      "   macro avg       0.63      0.57      0.58     39602\n",
      "weighted avg       0.87      0.89      0.88     39602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = (np.concatenate([torch.sigmoid(gcn(data.x,data.edge_index)).detach().flatten().tolist() \n",
    "                        for data in train_set[:100]]\n",
    "                      )>.5).astype(int)\n",
    "target = np.concatenate([data.y.flatten().tolist() for data in train_set[:100]])\n",
    "print(classification_report(target,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.97      0.93       536\n",
      "         1.0       0.10      0.03      0.05        67\n",
      "\n",
      "    accuracy                           0.86       603\n",
      "   macro avg       0.49      0.50      0.49       603\n",
      "weighted avg       0.80      0.86      0.83       603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = (np.concatenate([torch.sigmoid(gcn(data.x,data.edge_index)).detach().flatten().tolist() \n",
    "                        for data in val_set[:2]]\n",
    "                      )>.5).astype(int)\n",
    "target = np.concatenate([data.y.flatten().tolist() for data in val_set[:2]])\n",
    "print(classification_report(target,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
